{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단층 GRU 기반 생성 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vocabulary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듬 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"토큰 리스트를 Vocabulary에 추가\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"토큰에 대응하는 인덱스를 추출\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, tokenizer, token_to_idx=None):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        # KLUE RoBERTa 토크나이저에서 특수 토큰 설정\n",
    "        self._mask_token = tokenizer.mask_token\n",
    "        self._unk_token = tokenizer.unk_token\n",
    "        self._begin_seq_token = tokenizer.cls_token\n",
    "        self._end_seq_token = tokenizer.sep_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        # KLUE RoBERTa 토크나이저 초기화\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "        \n",
    "        # token_to_idx만 추출하여 부모 클래스 초기화\n",
    "        token_to_idx = contents.get('token_to_idx', {})\n",
    "        return cls(tokenizer=tokenizer, token_to_idx=token_to_idx)\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출\n",
    "        토큰이 없으면 UNK 인덱스를 반환\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 함함\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentVectorizer(object):\n",
    "    \"\"\"어휘 사전을 생성하고 관리\"\"\"\n",
    "    \n",
    "    def __init__(self, text_vocab, tokenizer, target_vocab=None):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            text_vocab (SequenceVocabulary): 댓글 텍스트의 어휘 사전\n",
    "            target_vocab (Vocabulary, optional): 타겟 레이블의 어휘 사전\n",
    "            tokenizer (AutoTokenizer, optional): 토큰화를 위한 RoBERTa 토크나이저\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_vocab = text_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, text, vector_length=-1):\n",
    "        \"\"\"댓글 텍스트를 입력 벡터와 출력 벡터로 변환\n",
    "        \n",
    "        매개변수:\n",
    "            text (str): 벡터로 변환할 댓글 텍스트\n",
    "            vector_length (int): 벡터의 고정 길이 (기본값: -1, 즉 가변 길이)\n",
    "        반환값:\n",
    "            tuple: (from_vector, to_vector)\n",
    "                from_vector (numpy.ndarray): 입력 텍스트의 벡터화 결과\n",
    "                to_vector (numpy.ndarray): 타겟 텍스트의 벡터화 결과\n",
    "        \"\"\"\n",
    "        # RoBERTa tokenizer로 토큰화\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "  \n",
    "        indices = [self.text_vocab.begin_seq_index]\n",
    "        indices.extend(self.text_vocab.lookup_token(token) for token in tokens)\n",
    "        indices.append(self.text_vocab.end_seq_index)\n",
    "        \n",
    "        from_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.text_vocab.mask_index\n",
    "\n",
    "        to_vector = np.zeros(vector_length,dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.text_vocab.mask_index\n",
    " \n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        \"\"\"데이터프레임을 이용해 CommentVectorizer 객체를 초기화\n",
    "        \n",
    "        매개변수:\n",
    "            df (pandas.DataFrame): 댓글 데이터프레임\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "        text_vocab = SequenceVocabulary(tokenizer=tokenizer)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            tokens = tokenizer.tokenize(row.text)\n",
    "            for char in tokens:\n",
    "                text_vocab.add_token(char)\n",
    "\n",
    "        target_vocab = None\n",
    "        if 'target_label' in df.columns:\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                target_vocab.add_token(row.target_label)\n",
    "\n",
    "        return cls(text_vocab, tokenizer, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"직렬화된 데이터를 사용해 CommentVectorizer 객체를 복원\n",
    "        \n",
    "        매개변수:\n",
    "            contents (dict): 직렬화된 데이터\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        # KLUE RoBERTa 토크나이저 초기화\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "        text_vocab = SequenceVocabulary.from_serializable(contents['text_vocab'])\n",
    "\n",
    "        # 타겟 어휘 사전 복원\n",
    "        target_vocab = None\n",
    "        if 'target_vocab' in contents:\n",
    "            target_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "\n",
    "        return cls(text_vocab=text_vocab, target_vocab=target_vocab, tokenizer=tokenizer)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"CommentVectorizer 객체를 직렬화 가능한 형태로 변환\n",
    "        \n",
    "        반환값:\n",
    "            dict: 직렬화된 데이터\n",
    "        \"\"\"\n",
    "        contents = {\n",
    "            'text_vocab': self.text_vocab.to_serializable()\n",
    "        }\n",
    "        if self.target_vocab:\n",
    "            contents['target_vocab'] = self.target_vocab.to_serializable()\n",
    "\n",
    "        return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, comment_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            comment_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (CommentVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
    "        \"\"\"\n",
    "        self.comment_df = comment_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # 최대 시퀀스 길이 계산\n",
    "        self._max_seq_length = max(len(self._vectorizer.tokenizer.tokenize(text)) \n",
    "                                 for text in self.comment_df.text) + 2\n",
    "\n",
    "        # 데이터셋 분할\n",
    "        self.train_df = self.comment_df[self.comment_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.comment_df[self.comment_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.comment_df[self.comment_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.validation_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer를 만듭니다.\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            CommentDataset 객체\n",
    "        \"\"\"\n",
    "        comment_df = pd.read_csv(dataset_csv)\n",
    "        return cls(comment_df, CommentVectorizer.from_dataframe(comment_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\n",
    "        \"\"\"데이터셋과 캐싱된 Vectorizer 객체를 로드합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            CommentDataset 객체\n",
    "        \"\"\"\n",
    "        comment_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(comment_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CommentVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"벡터 변환 객체를 반환합니다\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"데이터셋의 분할을 설정합니다.\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\"\"\"\n",
    "\n",
    "        row = self._target_df.iloc[index]\n",
    "        from_vector, to_vector = self._vectorizer.vectorize(row.text, self._max_seq_length)\n",
    "\n",
    "        return_dict = {\n",
    "            'x_data': from_vector,\n",
    "            'y_target': to_vector\n",
    "        }\n",
    "\n",
    "        # target_label 컬럼이 있고 vectorizer가 target_vocab을 가지고 있을 때만 처리\n",
    "        if 'target_label' in self.comment_df.columns and hasattr(self._vectorizer, 'target_vocab') and self._vectorizer.target_vocab is not None:\n",
    "      \n",
    "            target_index = self._vectorizer.target_vocab.lookup_token(row.target_label)\n",
    "            return_dict[\"target_index\"] = target_index\n",
    "\n",
    "        return return_dict\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"파이토치 DataLoader를 감싸고 있는 제너레이터 함수\"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                          shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단층 GRU 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerGRUModel(nn.Module):\n",
    "    \"\"\"단층 GRU 기반의 생성 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_embeddings, hidden_size, dropout_p,\n",
    "                 embedding_path, num_targets=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            embedding_dim (int): KLUE RoBERTa 임베딩 차원\n",
    "            num_embeddings (int): 임베딩 테이블 크기 (단어장 크기)\n",
    "            hidden_size (int): GRU의 은닉 상태 크기\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "            embedding_path (str): 정렬된 임베딩 파일 경로\n",
    "            num_targets (int, optional): 타겟 개수 (조건부 생성 시 사용)\n",
    "            padding_idx (int): 패딩 토큰의 인덱스\n",
    "        \"\"\"\n",
    "        super(SingleLayerGRUModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.load(embedding_path),\n",
    "            freeze=True,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        if num_targets is not None:\n",
    "   \n",
    "            self.target_emb = nn.Embedding(num_embeddings=num_targets,\n",
    "                                       embedding_dim=hidden_size)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size = embedding_dim,\n",
    "                          hidden_size = hidden_size,\n",
    "                          batch_first= True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=hidden_size,\n",
    "                            out_features=num_embeddings)\n",
    "\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "        self.has_target = num_targets is not None  # 조건부 생성 여부 확인\n",
    "\n",
    "    def forward(self, x_in, target_index=None, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 텐서 (batch_size, sequence_length)\n",
    "            target_index (torch.Tensor, optional): 타겟 텐서 (조건부 생성 시 사용)\n",
    "            apply_softmax (bool): 소프트맥스 적용 여부\n",
    "        반환값:\n",
    "            output (torch.Tensor): 출력 텐서 (batch_size, sequence_length, num_embeddings)\n",
    "        \"\"\"\n",
    "        # 입력 텍스트 임베딩\n",
    "\n",
    "        x_embedded = self.embedding(x_in)\n",
    "\n",
    "        if self.has_target and target_index is not None:\n",
    "   \n",
    "            target_embedded = self.target_emb(target_index).unsqueeze(0)\n",
    "            y_out, _ = self.rnn(x_embedded,target_embedded)\n",
    "   \n",
    "        else:\n",
    "\n",
    "            y_out, _ = self.rnn(x_embedded)\n",
    "    \n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out =y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out= self.fc(F.dropout(y_out,p=self._dropout_p))\n",
    "                               \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "     \n",
    "        output = y_out.view(batch_size, seq_size, new_feat_size)    \n",
    "  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"텐서 크기 정규화\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 2차원 행렬로 변환\n",
    "        y_true (torch.Tensor): 타깃 텐서\n",
    "            2차원 텐서이면 1차원 벡터로 변환\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    \"\"\"정확도 계산\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        정확도 (float)\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    \"\"\"시퀀스 손실 계산\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        손실 값 (torch.Tensor)\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "def compute_perplexity(y_pred, y_true, mask_index):\n",
    "    \"\"\"Perplexity 계산 함수\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        perplexity (float): Perplexity 값\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    loss = F.cross_entropy(y_pred, y_true, ignore_index=mask_index, reduction='sum')\n",
    "    n_tokens = torch.ne(y_true, mask_index).sum().item()\n",
    "    perplexity = math.exp(loss.item() / n_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, prompts, targets=None, sample_size=50, \n",
    "                    temperature=1.0):\n",
    "   \"\"\"모델이 만든 인덱스 시퀀스를 샘플링\n",
    "\n",
    "   매개변수:\n",
    "       model: 훈련된 모델\n",
    "       vectorizer: CommentVectorizer 객체  \n",
    "       prompts (list): 생성 시작 부분 텍스트 리스트\n",
    "       targets (list): 각 샘플의 target(str)을 나타내는 리스트 (조건부 생성시 사용)\n",
    "       sample_size (int): 샘플의 최대 길이\n",
    "       temperature (float): 무작위성 정도\n",
    "           0.0 < temperature < 1.0 이면 최대 값을 선택할 가능성이 높습니다\n",
    "           temperature > 1.0 이면 균등 분포에 가깝습니다\n",
    "\n",
    "   반환값:\n",
    "       indices (torch.Tensor): 인덱스 행렬 (batch_size, sample_size)\n",
    "   \"\"\"\n",
    "   begin_seq_indices = []\n",
    "   for prompt in prompts:\n",
    "       tokens = vectorizer.tokenizer.tokenize(prompt)\n",
    "       indices = [vectorizer.text_vocab.begin_seq_index]\n",
    "       indices.extend(vectorizer.text_vocab.lookup_token(token) for token in tokens)\n",
    "       begin_seq_indices.append(indices)\n",
    "   \n",
    "   max_len = max(len(indices) for indices in begin_seq_indices)\n",
    "   for indices in begin_seq_indices:\n",
    "       while len(indices) < max_len:\n",
    "           indices.append(vectorizer.text_vocab.mask_index)\n",
    "   \n",
    "   indices = torch.tensor(begin_seq_indices, dtype=torch.int64)\n",
    "   \n",
    "   target_tensor = None\n",
    "   if targets is not None and hasattr(vectorizer, 'target_vocab'):\n",
    "       target_indices = [vectorizer.target_vocab.lookup_token(target) for target in targets]\n",
    "       target_tensor = torch.tensor(target_indices, dtype=torch.int64)\n",
    "   \n",
    "   generated = indices\n",
    "   for _ in range(sample_size - max_len):\n",
    "       if target_tensor is not None:\n",
    "           y_pred = model(generated, target_tensor, apply_softmax=True)\n",
    "       else:\n",
    "           y_pred = model(generated, apply_softmax=True)\n",
    "           \n",
    "       next_token_logits = y_pred[:, -1, :] / temperature\n",
    "       next_token = torch.multinomial(next_token_logits, num_samples=1)\n",
    "       generated = torch.cat([generated, next_token], dim=1)\n",
    "       \n",
    "       if (next_token == vectorizer.text_vocab.end_seq_index).any():\n",
    "           break\n",
    "   \n",
    "   return generated\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    \"\"\"인덱스를 댓글 문자열로 변환\n",
    "\n",
    "    매개변수:\n",
    "        sampled_indices (torch.Tensor): sample_from_model 함수에서 얻은 인덱스 \n",
    "        vectorizer (CommentVectorizer): CommentVectorizer 객체\n",
    "\n",
    "    반환값:\n",
    "        decoded_comments (list): 디코딩된 댓글 문자열의 리스트\n",
    "    \"\"\"\n",
    "    decoded_comments = []\n",
    "    vocab = vectorizer.text_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        text = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                text += vocab.lookup_index(sample_item)\n",
    "        decoded_comments.append(text)\n",
    "       \n",
    "    return decoded_comments\n",
    "\n",
    "def load_models_and_vectorizers():\n",
    "    models = {}\n",
    "    vectorizers = {}\n",
    "    \n",
    "    model_paths = {\n",
    "        'normal': Path('model_storage/normal_comment_generation'),\n",
    "        'hate': Path('model_storage/hate_comment_generation'),\n",
    "        'conditioned_hate': Path('model_storage/conditioned_hate_comment_generation')\n",
    "    }\n",
    "    \n",
    "    for model_type, path in model_paths.items():\n",
    "        with open(path / 'vectorizer.json', 'r') as f:\n",
    "            vectorizer_data = json.load(f)\n",
    "            vectorizers[model_type] = CommentVectorizer.from_serializable(vectorizer_data)\n",
    "        \n",
    "        model_path = path / 'single_gru_model.pth'\n",
    "        if model_type == 'conditioned_hate':\n",
    "            models[model_type] = SingleLayerGRUModel(\n",
    "                embedding_dim=768,\n",
    "                num_embeddings=len(vectorizers[model_type].text_vocab),\n",
    "                num_targets=len(vectorizers[model_type].target_vocab),\n",
    "                dropout_p=0.1,\n",
    "                hidden_size=512,\n",
    "                embedding_path=str(path / 'roberta_embedding.pt'),\n",
    "                padding_idx=vectorizers[model_type].text_vocab.mask_index\n",
    "            )\n",
    "        else:\n",
    "            models[model_type] = SingleLayerGRUModel(\n",
    "                embedding_dim=768,\n",
    "                num_embeddings=len(vectorizers[model_type].text_vocab),\n",
    "                dropout_p=0.1,\n",
    "                hidden_size=512,\n",
    "                embedding_path=str(path / 'roberta_embedding.pt'),\n",
    "                padding_idx=vectorizers[model_type].text_vocab.mask_index\n",
    "            )\n",
    "        \n",
    "        models[model_type].load_state_dict(torch.load(model_path))\n",
    "        models[model_type].eval()\n",
    "    \n",
    "    return models, vectorizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 정량적 성능 평가 시작 ===\n",
      "\n",
      "1. 테스트 데이터셋 로드 중...\n",
      "\n",
      "일반 댓글 데이터셋 로드 중...\n",
      "\n",
      "혐오 댓글 데이터셋 로드 중...\n",
      "\n",
      "2. 모델 로드 및 평가 시작...\n",
      "\n",
      "일반 댓글 테스트셋 평가:\n",
      "============================================================\n",
      "\n",
      "=== normal 모델 평가 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "테스트 중: 100%|██████████| 31/31 [01:46<00:00,  3.43s/it, acc=61.8, loss=1.64, ppl=5.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== hate 모델 평가 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "테스트 중: 100%|██████████| 31/31 [01:39<00:00,  3.21s/it, acc=0.313, loss=16.8, ppl=2.09e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "혐오 댓글 테스트셋 평가:\n",
      "============================================================\n",
      "\n",
      "=== normal 모델 평가 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "테스트 중: 100%|██████████| 31/31 [02:20<00:00,  4.54s/it, acc=0.463, loss=18.6, ppl=1.23e+8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== hate 모델 평가 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "테스트 중: 100%|██████████| 31/31 [02:22<00:00,  4.59s/it, acc=56.2, loss=1.96, ppl=7.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== conditioned_hate 모델 평가 중 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "테스트 중: 100%|██████████| 31/31 [02:31<00:00,  4.88s/it, acc=39.3, loss=3.15, ppl=23.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. 평가 결과 요약\n",
      "\n",
      "=== 모델 성능 비교 ===\n",
      "\n",
      "[일반 댓글 테스트셋 결과]\n",
      "================================================================================\n",
      "         모델              Loss       Accuracy    Perplexity \n",
      "================================================================================\n",
      "       normal           1.6373       61.79         5.15    \n",
      "        hate           16.8153        0.31     20870141.19 \n",
      "================================================================================\n",
      "\n",
      "[혐오 댓글 테스트셋 결과]\n",
      "================================================================================\n",
      "         모델              Loss       Accuracy    Perplexity \n",
      "================================================================================\n",
      "       normal          18.6069        0.46     123459010.18\n",
      "        hate            1.9569       56.16         7.09    \n",
      "  conditioned_hate      3.1510       39.29        23.77    \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 정량적 성능 평가 시작 ===\")\n",
    "\n",
    "def evaluate_model_on_testset(model, dataset, vectorizer, target_label=None):\n",
    "    \"\"\"특정 데이터셋에 대한 모델의 성능을 평가\"\"\"\n",
    "    model = model.eval()\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, batch_size=64, device='cpu')\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_ppl = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    test_bar = tqdm(desc='테스트 중', \n",
    "                   total=dataset.get_num_batches(64),\n",
    "                   position=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch_dict in enumerate(batch_generator):\n",
    "            # 모델 예측\n",
    "            if target_label is not None and hasattr(model, 'has_target'):\n",
    "                target_indices = torch.tensor([\n",
    "                    vectorizer.target_vocab.lookup_token(target_label)\n",
    "                ] * batch_dict['x_data'].size(0))\n",
    "                y_pred = model(x_in=batch_dict['x_data'], target_index=target_indices)\n",
    "            else:\n",
    "                y_pred = model(x_in=batch_dict['x_data'])\n",
    "            \n",
    "            # 평가 지표 계산\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'],\n",
    "                               vectorizer.text_vocab.mask_index)\n",
    "            acc = compute_accuracy(y_pred, batch_dict['y_target'],\n",
    "                                 vectorizer.text_vocab.mask_index)\n",
    "            ppl = compute_perplexity(y_pred, batch_dict['y_target'],\n",
    "                                   vectorizer.text_vocab.mask_index)\n",
    "            \n",
    "            # 통계 업데이트\n",
    "            batch_count += 1\n",
    "            running_loss += (loss.item() - running_loss) / batch_count\n",
    "            running_acc += (acc - running_acc) / batch_count\n",
    "            running_ppl += (ppl - running_ppl) / batch_count\n",
    "            \n",
    "            test_bar.set_postfix(loss=running_loss, acc=running_acc, ppl=running_ppl)\n",
    "            test_bar.update()\n",
    "    \n",
    "    test_bar.close()\n",
    "    return {\n",
    "        'loss': running_loss,\n",
    "        'accuracy': running_acc,\n",
    "        'perplexity': running_ppl\n",
    "    }\n",
    "\n",
    "print(\"\\n1. 테스트 데이터셋 로드 중...\")\n",
    "\n",
    "# 일반 댓글 데이터셋\n",
    "print(\"\\n일반 댓글 데이터셋 로드 중...\")\n",
    "normal_dataset = CommentDataset.load_dataset_and_load_vectorizer(\n",
    "    \"normal_comments.csv\",\n",
    "    \"model_storage/normal_comment_generation/vectorizer.json\"\n",
    ")\n",
    "\n",
    "# 혐오 댓글 데이터셋\n",
    "print(\"\\n혐오 댓글 데이터셋 로드 중...\")\n",
    "hate_dataset = CommentDataset.load_dataset_and_load_vectorizer(\n",
    "    \"hate_comments.csv\",\n",
    "    \"model_storage/hate_comment_generation/vectorizer.json\"\n",
    ")\n",
    "\n",
    "print(\"\\n2. 모델 로드 및 평가 시작...\")\n",
    "models, vectorizers = load_models_and_vectorizers()\n",
    "\n",
    "results = {\n",
    "    'normal_testset': {},\n",
    "    'hate_testset': {}\n",
    "}\n",
    "\n",
    "# 각 모델의 성능 평가\n",
    "print(\"\\n일반 댓글 테스트셋 평가:\")\n",
    "print(\"=\"*60)\n",
    "for model_type in ['normal', 'hate']:  # 조건부 모델 제외\n",
    "    print(f\"\\n=== {model_type} 모델 평가 중 ===\")\n",
    "    results['normal_testset'][model_type] = evaluate_model_on_testset(\n",
    "        models[model_type], normal_dataset, vectorizers['normal']\n",
    "    )\n",
    "\n",
    "print(\"\\n혐오 댓글 테스트셋 평가:\")\n",
    "print(\"=\"*60)\n",
    "for model_type in ['normal', 'hate', 'conditioned_hate']:  # 모든 모델 포함\n",
    "    print(f\"\\n=== {model_type} 모델 평가 중 ===\")\n",
    "    if model_type == 'conditioned_hate':\n",
    "        target_label = hate_dataset.test_df.iloc[0].target_label\n",
    "        results['hate_testset'][model_type] = evaluate_model_on_testset(\n",
    "            models[model_type], \n",
    "            hate_dataset,\n",
    "            vectorizers['conditioned_hate'],\n",
    "            target_label=target_label\n",
    "        )\n",
    "    else:\n",
    "        results['hate_testset'][model_type] = evaluate_model_on_testset(\n",
    "            models[model_type], \n",
    "            hate_dataset, \n",
    "            vectorizers['hate']\n",
    "        )\n",
    "\n",
    "print(\"\\n3. 평가 결과 요약\")\n",
    "print(\"\\n=== 모델 성능 비교 ===\")\n",
    "\n",
    "# 일반 댓글 테스트셋 결과\n",
    "print(\"\\n[일반 댓글 테스트셋 결과]\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'모델':^20} {'Loss':^12} {'Accuracy':^12} {'Perplexity':^12}\")\n",
    "print(\"=\"*80)\n",
    "for model_name, metrics in results['normal_testset'].items():\n",
    "    print(f\"{model_name:^20} {metrics['loss']:^12.4f} \"\n",
    "          f\"{metrics['accuracy']:^12.2f} {metrics['perplexity']:^12.2f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 혐오 댓글 테스트셋 결과\n",
    "print(\"\\n[혐오 댓글 테스트셋 결과]\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'모델':^20} {'Loss':^12} {'Accuracy':^12} {'Perplexity':^12}\")\n",
    "print(\"=\"*80)\n",
    "for model_name, metrics in results['hate_testset'].items():\n",
    "    print(f\"{model_name:^20} {metrics['loss']:^12.4f} \"\n",
    "          f\"{metrics['accuracy']:^12.2f} {metrics['perplexity']:^12.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "test_samples = pd.DataFrame({\n",
    "    'text': [\n",
    "        # 일반 댓글 (1-5)\n",
    "        \"이름 장군님께서 만드신 체육이다 늘 항상 이름 장군님께도 감사하게 여기며 살아라 축하한다\",\n",
    "        \"이름 오빠 믿어!!!!!!! 항상 믿어요!!!!!!! 영원히 함께하자!!! 사랑해요\",\n",
    "        \"못믿겠는데  숏컷한 여자들이 대한민국에 몇명인데  중년이상 여성은 반이상이 숏컷인데 저걸 뭐라했을 리가 없다 \",\n",
    "        \"사실상 통일이 가능한가? 가능하더라도 해야할까? 둘다 아니라고 본다\",\n",
    "        \"세상에 이미 죽은 사람에게 상을 주다니 이게 말이 되냐?\",\n",
    "        # 혐오 댓글 (6-10)\n",
    "        \"50대 이상 오래 살았다 아제 맞고 죽어라 \",  # age\n",
    "        \"정치판 기웃대다가  떡실신되서 수술도 안되는 이름발면지켜보는 접대 못해서 안달난 접대녀  우리 이름도 있다\",  # gender\n",
    "        \"서울 분리독립하자  외국인들 특히 홍어 짱깨 조선족들 지네나라로 강제추방해버리자 \",  # region\n",
    "        \"미친거 아니야? 민주당 자체가 공산주의를 동경하는구나 나라 망치지말고 북으로 가라 빨갱이들아\",  # political\n",
    "        \"체조복장을 성적대상화로 보는 정신이상자가 있나… ? 왜 지들끼리 육갑떨지… ?\",  # disabled\n",
    "    ],\n",
    "    'prompt': [\n",
    "        # 일반 댓글 프롬프트\n",
    "        \"이름 장군님께서 만드신 체육이다 늘 항상 \",\n",
    "        \"이름 오빠 믿어!!!!!!! 항상 \",\n",
    "        \"못믿겠는데  숏컷한 여자들이 대한민국에 몇명인데 \",\n",
    "        \"사실상 통일이 가능한가? 가능하더라도 \",\n",
    "        \"세상에 이미 죽은 사람에게 상을 주다니 \",\n",
    "        # 혐오 댓글 프롬프트 (앞부분만)\n",
    "        \"50대 이상 \",\n",
    "        \"정치판 기웃대다가  떡실신되서 수술도 안되는 \",\n",
    "        \"서울 분리독립하자  외국인들 특히 \",\n",
    "        \"미친거 아니야? 민주당 자체가 \",\n",
    "        \"체조복장을 성적대상화로 보는 \"\n",
    "    ],\n",
    "    'target_label': [\n",
    "        # 일반 댓글은 타겟 없음\n",
    "        None, None, None, None, None,\n",
    "        # 혐오 댓글 타겟\n",
    "        \"['age']\",\n",
    "        \"['gender']\",\n",
    "        \"['region']\",\n",
    "        \"['political']\",\n",
    "        \"['disabled']\"\n",
    "    ],\n",
    "    'is_hate': [\n",
    "        # 댓글 타입 구분\n",
    "        False, False, False, False, False,\n",
    "        True, True, True, True, True\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 모델과 vectorizer 로드 중...\n",
      "\n",
      "2. 댓글 이어쓰기 테스트...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 1]\n",
      "원본 댓글: 이름 장군님께서 만드신 체육이다 늘 항상 이름 장군님께도 감사하게 여기며 살아라 축하한다\n",
      "\n",
      "입력된 댓글 앞 부분: 이름 장군님께서 만드신 체육이다 늘 항상 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "이름장군##님##께##서만드##신체육##이다늘항상이름장군##님##께##서알##면서든든##하##게살##면서다른모습##으로보##기좋##습##니다축하##드##님너무큰감동##이그래도국##힘##에국##구##를많이합니다결국대한민국\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "이름장군##님##께##서만드##신체육##이다늘항상꽃##을##듯죄##다!!!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 2]\n",
      "원본 댓글: 이름 오빠 믿어!!!!!!! 항상 믿어요!!!!!!! 영원히 함께하자!!! 사랑해요\n",
      "\n",
      "입력된 댓글 앞 부분: 이름 오빠 믿어!!!!!!! 항상 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "이름오빠믿##어!!!!!!!항상이게##교육믿##고!!!!믿##고##자##나!!!!믿##고!!!!!이름친구!!!같##은여론##으로##불선동##하##지!\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "이름오빠믿##어!!!!!!!항상몰락##한놈##이니##가아니##고대선##까##지해라~\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 3]\n",
      "원본 댓글: 못믿겠는데  숏컷한 여자들이 대한민국에 몇명인데  중년이상 여성은 반이상이 숏컷인데 저걸 뭐라했을 리가 없다 \n",
      "\n",
      "입력된 댓글 앞 부분: 못믿겠는데  숏컷한 여자들이 대한민국에 몇명인데 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "못##믿##겠##는데숏##컷##한여자##들이대한민국##에몇##명##인##데중년##이상여성##은일본##에서##도숏##컷##이심각##했##다남자##고숏##컷잡##은사람##들##의명##이다\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "못##믿##겠##는데숏##컷##한여자##들이대한민국##에몇##명##인##데무슨무##가##가많##을##게[UNK]오##쓰[UNK]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 4]\n",
      "원본 댓글: 사실상 통일이 가능한가? 가능하더라도 해야할까? 둘다 아니라고 본다\n",
      "\n",
      "입력된 댓글 앞 부분: 사실상 통일이 가능한가? 가능하더라도 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "사실상통일##이가능##한##가?가능##하##더라##도아직못##맞##는데어디##서못##받##을캐##는##거##야?그것##들##은정치##질##하##냐?누구##는##사람##이정치##하##는말##이되##면정치##방##역##이목적\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "사실상통일##이가능##한##가?가능##하##더라##도북##정##치##들##을죽##었##어도정치인##들이되##기##도하##는놈##들이##가넘##이##거##냐?ㅋㅋ잘##맞##고생각##이나한##데뭐##하##는짓##지ㅋㅋ\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 5]\n",
      "원본 댓글: 세상에 이미 죽은 사람에게 상을 주다니 이게 말이 되냐?\n",
      "\n",
      "입력된 댓글 앞 부분: 세상에 이미 죽은 사람에게 상을 주다니 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "세상##에이미죽##은사람##에##게상##을주##다니이게말##이다!!!본인##들아이름##이!!!\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "세상##에이미죽##은사람##에##게상##을주##다니##며막저렇게자가있##을까##요그런인간##들이국회의원##은해야##돼##겠##니업적##한넘##어##도이름##내##를보##기맞##나?죄인##이##거나평생##인대통령##으##신\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 6]\n",
      "원본 댓글: 50대 이상 오래 살았다 아제 맞고 죽어라 \n",
      "타겟: age\n",
      "\n",
      "입력된 댓글 앞 부분: 50대 이상 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "50##대이상아재##들부럽##다\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "50##대이상##한##텐##데한마디##로백신##을무력##하라##해?\n",
      "\n",
      "[조건부 혐오 댓글 모델 (타겟: age)]\n",
      "50##대이상오래##다##놓##고어##분##생##이나여자##들##은다아##는얘기##잖아?하나못디지버##러가##는데그래서##다부모부모부모##는무슨##일##화##햇##는지ㅋ\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 7]\n",
      "원본 댓글: 정치판 기웃대다가  떡실신되서 수술도 안되는 이름발면지켜보는 접대 못해서 안달난 접대녀  우리 이름도 있다\n",
      "타겟: gender\n",
      "\n",
      "입력된 댓글 앞 부분: 정치판 기웃대다가  떡실신되서 수술도 안되는 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "정치판기웃##대##다가떡##실##신##되##서수술##도안##되##는기사##보##며물##어##서패##받##는쪽##로쪽##로##네##요\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "정치판기웃##대##다가떡##실##신##되##서수술##도안##되##는것##들없##지묻##지왜안##받##았##냐?내##로##남##불\n",
      "\n",
      "[조건부 혐오 댓글 모델 (타겟: gender)]\n",
      "정치판기웃##대##다가떡##실##신##되##서수술##도안##되##는그썩##어##있##어##디##로##서##해서그냥살##게만들##어##준##게나##발##을하##는##거지한국##에##선일##하##다찍##으면꼴##통##들##은이\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 8]\n",
      "원본 댓글: 서울 분리독립하자  외국인들 특히 홍어 짱깨 조선족들 지네나라로 강제추방해버리자 \n",
      "타겟: region\n",
      "\n",
      "입력된 댓글 앞 부분: 서울 분리독립하자  외국인들 특히 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "서울분리##독##립##하##자외국인##들특히사과##하면그나라##가싫어?탈집권##하##던##지시킨##거##다정부##는말##로적극##으로해서정권##교##체##하면되##이다그런##데여론##을보고##시##켜??\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "서울분리##독##립##하##자외국인##들특히홍어냄새나##네좌파##들##아니##가???코##로나##로모여서재앙##이##빨##이##하##냐?ㅋㅋ\n",
      "\n",
      "[조건부 혐오 댓글 모델 (타겟: region)]\n",
      "서울분리##독##립##하##자외국인##들특히홍어##들서울경기##는침몰##시##켜아주목##아##파##구나\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 9]\n",
      "원본 댓글: 미친거 아니야? 민주당 자체가 공산주의를 동경하는구나 나라 망치지말고 북으로 가라 빨갱이들아\n",
      "타겟: political\n",
      "\n",
      "입력된 댓글 앞 부분: 미친거 아니야? 민주당 자체가 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "미친##거아니야?민주당자체##가아니##잖##어대학##으로##도안##되##는##거아니##냐?\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "미친##거아니야?민주당자체##가썩##었##네?우리##는개##독##과토##해##냐?ㅈ##도지##는가라??\n",
      "\n",
      "[조건부 혐오 댓글 모델 (타겟: political)]\n",
      "미친##거아니야?민주당자체##가공산주의##를동경##해##왔##다문정##권##은잘##하##는##가아니##라문정##부##는더##치##지않##고남##의나라##에서몽둥이##로가##고##온나라##의미래##가있##다헛소리해라빨갱이##를\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[샘플 10]\n",
      "원본 댓글: 체조복장을 성적대상화로 보는 정신이상자가 있나… ? 왜 지들끼리 육갑떨지… ?\n",
      "타겟: disabled\n",
      "\n",
      "입력된 댓글 앞 부분: 체조복장을 성적대상화로 보는 \n",
      "\n",
      "생성 결과:\n",
      "----------------------------------------\n",
      "[일반 댓글 모델]\n",
      "체조##복##장##을성적##대##상##화##로보##는##것##은대체학##교육##이아니##라괜히##보##고야##변##의힘##도자질그대로##인##가하##기전##에생각##을해도못하##지##않##길당대##표##가몸##이아닐##것\n",
      "\n",
      "[혐오 댓글 모델]\n",
      "체조##복##장##을성적##대##상##화##로보##는인간##이##1##일##로##데왜##가했##을##건##지?정권##은대체##가답##이지##만같##은살인자##자##개##준일##이지\n",
      "\n",
      "[조건부 혐오 댓글 모델 (타겟: disabled)]\n",
      "체조##복##장##을성적##대##상##화##로보##는##구나~^~\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. 모델과 vectorizer 로드 중...\")\n",
    "models, vectorizers = load_models_and_vectorizers()\n",
    "\n",
    "print(\"\\n2. 댓글 이어쓰기 테스트...\")\n",
    "for idx, sample in enumerate(test_samples.itertuples(), start=1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\n[샘플 {idx}]\")\n",
    "\n",
    "    text = sample.text\n",
    "    prompt = sample.prompt\n",
    "    is_hate = sample.is_hate\n",
    "\n",
    "    print(f\"원본 댓글: {text}\")\n",
    "    if is_hate:\n",
    "        target = eval(sample.target_label)[0]\n",
    "        print(f\"타겟: {target}\")\n",
    "    print(f\"\\n입력된 댓글 앞 부분: {prompt}\")\n",
    "    print(\"\\n생성 결과:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 일반 댓글 모델\n",
    "    model = models['normal'].cpu()\n",
    "    sampled_indices = sample_from_model(\n",
    "        model,\n",
    "        vectorizers['normal'],\n",
    "        prompts=[prompt],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    normal_result = decode_samples(sampled_indices, vectorizers['normal'])[0]\n",
    "    print(f\"[일반 댓글 모델]\\n{normal_result}\\n\")\n",
    "\n",
    "    # 혐오 댓글 모델\n",
    "    model = models['hate'].cpu()\n",
    "    sampled_indices = sample_from_model(\n",
    "        model,\n",
    "        vectorizers['hate'],\n",
    "        prompts=[prompt],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    hate_result = decode_samples(sampled_indices, vectorizers['hate'])[0]\n",
    "    print(f\"[혐오 댓글 모델]\\n{hate_result}\\n\")\n",
    "\n",
    "    # 혐오 댓글인 경우에만 조건부 모델 실행\n",
    "    if is_hate:\n",
    "        model = models['conditioned_hate'].cpu()\n",
    "        sampled_indices = sample_from_model(\n",
    "            model,\n",
    "            vectorizers['conditioned_hate'],\n",
    "            prompts=[prompt],\n",
    "            targets=[target],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        conditioned_result = decode_samples(sampled_indices, vectorizers['conditioned_hate'])[0]\n",
    "        print(f\"[조건부 혐오 댓글 모델 (타겟: {target})]\\n{conditioned_result}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
